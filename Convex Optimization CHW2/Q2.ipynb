{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Name = 'Parsa Ghezelbash'\n",
    "Student_ID = '401110437'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg as la\n",
    "import cvxpy as cp\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\min_{x} \\quad \\frac{1}{2} \\|Ax - b\\|_2^2 + \\gamma \\|x\\|_1\n",
    "$$\n",
    "\n",
    "- \\( $A \\in \\mathbb{R}^{m \\times n}$ \\) is the input matrix.\n",
    "- \\( $b \\in \\mathbb{R}^{m}$ \\) is the target vector.\n",
    "- \\( $\\gamma > 0$ \\) is the regularization parameter that controls sparsity.\n",
    "\n",
    "### Proximal Gradient Descent\n",
    "$$\n",
    "\\text{prox}_{\\lambda}(y) = \\text{sign}(y) \\cdot \\max(|y| - \\lambda, 0)\n",
    "$$\n",
    "\n",
    "The algorithm iterates until convergence or until a maximum number of iterations is reached. The step size is chosen based on the Lipschitz constant of \\( $A^TA$ \\).\n",
    "\n",
    "### ADMM for LASSO\n",
    "$$\n",
    "\\min_{x, y} \\quad \\frac{1}{2} \\|Ax - b\\|_2^2 + \\gamma \\|y\\|_1 \\quad \\text{subject to} \\quad x = y\n",
    "$$\n",
    "\n",
    "The augmented Lagrangian for this problem is given by:\n",
    "\n",
    "$$\n",
    "L(x, y, \\mu) = \\frac{1}{2} \\|Ax - b\\|_2^2 + \\gamma \\|y\\|_1 + \\mu^T (x - y) + \\frac{\\rho}{2} \\|x - y\\|_2^2\n",
    "$$\n",
    "\n",
    "where:\n",
    "- \\( $\\mu$ \\) is the dual variable (Lagrange multiplier).\n",
    "- \\( $\\rho > 0$ \\) is the penalty parameter.\n",
    "\n",
    "The ADMM algorithm consists of the following updates at each iteration:\n",
    "1. **\\( x \\)-update:** Solve the quadratic subproblem:\n",
    "   $$\n",
    "   x^{k+1} = \\arg\\min_{x} \\left( \\frac{1}{2} \\|Ax - b\\|_2^2 + \\frac{\\rho}{2} \\|x - y^k + \\frac{\\mu^k}{\\rho}\\|_2^2 \\right)\n",
    "   $$\n",
    "2. **\\( y \\)-update:** Apply the soft-thresholding operator:\n",
    "   $$\n",
    "   y^{k+1} = \\text{prox}_{\\frac{\\gamma}{\\rho}}\\left(x^{k+1} + \\frac{\\mu^k}{\\rho}\\right)\n",
    "   $$\n",
    "3. **\\( \\mu \\)-update:** Update the dual variable:\n",
    "   $$\n",
    "   \\mu^{k+1} = \\mu^k + \\rho (x^{k+1} - y^{k+1})\n",
    "   $$\n",
    "\n",
    "The iterations continue until convergence, which is typically determined by checking the primal and dual residuals:\n",
    "$$\n",
    "\\text{Primal residual:} \\quad r^k = \\|x^{k} - y^{k}\\|_2\n",
    "$$\n",
    "$$\n",
    "\\text{Dual residual:} \\quad s^k = \\rho \\|y^{k} - y^{k-1}\\|_2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating CVXPY for m=100, n=50\n",
      "CVXPY solution time (m=100, n=50): 0.0114 seconds\n",
      "CVXPY optimal value (m=100, n=50): 0.4833\n",
      "\n",
      "Evaluating Proximal Gradient for m=100, n=50\n",
      "Proximal Gradient solution time (m=100, n=50, stepsize=0.01): 0.0175 seconds\n",
      "Proximal Gradient optimal value (m=100, n=50, stepsize=0.01): 0.4833\n",
      "Proximal Gradient solution time (m=100, n=50, stepsize=0.05): 0.0088 seconds\n",
      "Proximal Gradient optimal value (m=100, n=50, stepsize=0.05): 0.4833\n",
      "Proximal Gradient solution time (m=100, n=50, stepsize=0.1): 0.0045 seconds\n",
      "Proximal Gradient optimal value (m=100, n=50, stepsize=0.1): 0.4833\n",
      "\n",
      "Evaluating ADMM for m=100, n=50\n",
      "ADMM solution time (m=100, n=50, rho=0.1): 0.0017 seconds\n",
      "ADMM optimal value (m=100, n=50, rho=0.1): 0.4833\n",
      "ADMM solution time (m=100, n=50, rho=1): 0.0007 seconds\n",
      "ADMM optimal value (m=100, n=50, rho=1): 0.4833\n",
      "ADMM solution time (m=100, n=50, rho=10): 0.0026 seconds\n",
      "ADMM optimal value (m=100, n=50, rho=10): 0.4833\n",
      "\n",
      "Evaluating CVXPY for m=200, n=100\n",
      "CVXPY solution time (m=200, n=100): 0.0206 seconds\n",
      "CVXPY optimal value (m=200, n=100): 0.5000\n",
      "\n",
      "Evaluating Proximal Gradient for m=200, n=100\n",
      "Proximal Gradient solution time (m=200, n=100, stepsize=0.01): 0.0006 seconds\n",
      "Proximal Gradient optimal value (m=200, n=100, stepsize=0.01): 0.5000\n",
      "Proximal Gradient solution time (m=200, n=100, stepsize=0.05): 0.0005 seconds\n",
      "Proximal Gradient optimal value (m=200, n=100, stepsize=0.05): 0.5000\n",
      "Proximal Gradient solution time (m=200, n=100, stepsize=0.1): 0.0017 seconds\n",
      "Proximal Gradient optimal value (m=200, n=100, stepsize=0.1): 0.5000\n",
      "\n",
      "Evaluating ADMM for m=200, n=100\n",
      "ADMM solution time (m=200, n=100, rho=0.1): 0.6246 seconds\n",
      "ADMM optimal value (m=200, n=100, rho=0.1): 0.5000\n",
      "ADMM solution time (m=200, n=100, rho=1): 0.5407 seconds\n",
      "ADMM optimal value (m=200, n=100, rho=1): 0.5000\n",
      "ADMM solution time (m=200, n=100, rho=10): 0.5403 seconds\n",
      "ADMM optimal value (m=200, n=100, rho=10): 0.5000\n",
      "\n",
      "Evaluating CVXPY for m=300, n=150\n",
      "CVXPY solution time (m=300, n=150): 0.0755 seconds\n",
      "CVXPY optimal value (m=300, n=150): 0.5000\n",
      "\n",
      "Evaluating Proximal Gradient for m=300, n=150\n",
      "Proximal Gradient solution time (m=300, n=150, stepsize=0.01): 0.0025 seconds\n",
      "Proximal Gradient optimal value (m=300, n=150, stepsize=0.01): 0.5000\n",
      "Proximal Gradient solution time (m=300, n=150, stepsize=0.05): 0.0023 seconds\n",
      "Proximal Gradient optimal value (m=300, n=150, stepsize=0.05): 0.5000\n",
      "Proximal Gradient solution time (m=300, n=150, stepsize=0.1): 0.0068 seconds\n",
      "Proximal Gradient optimal value (m=300, n=150, stepsize=0.1): 0.5000\n",
      "\n",
      "Evaluating ADMM for m=300, n=150\n",
      "ADMM solution time (m=300, n=150, rho=0.1): 0.5716 seconds\n",
      "ADMM optimal value (m=300, n=150, rho=0.1): 0.5000\n",
      "ADMM solution time (m=300, n=150, rho=1): 0.5481 seconds\n",
      "ADMM optimal value (m=300, n=150, rho=1): 0.5000\n",
      "ADMM solution time (m=300, n=150, rho=10): 0.5382 seconds\n",
      "ADMM optimal value (m=300, n=150, rho=10): 0.5000\n"
     ]
    }
   ],
   "source": [
    "def solve_with_cvxpy(m, n, gamma=0.1):\n",
    "    np.random.seed(0)\n",
    "    A = np.random.randn(m, n)\n",
    "    b = np.random.randn(m)\n",
    "    A = A / np.linalg.norm(A, ord=2)\n",
    "    b = b / np.linalg.norm(b, ord=2)\n",
    "\n",
    "    start_time = time.time()\n",
    "    x = cp.Variable(n)\n",
    "    loss = (1/2) * cp.norm2(A @ x - b)**2 + gamma * cp.norm1(x)\n",
    "    problem = cp.Problem(cp.Minimize(loss))\n",
    "    problem.solve()\n",
    "    cvxpy_time = time.time() - start_time\n",
    "\n",
    "    print(f\"CVXPY solution time (m={m}, n={n}): {cvxpy_time:.4f} seconds\")\n",
    "    print(f\"CVXPY optimal value (m={m}, n={n}): {problem.value:.4f}\")\n",
    "    return problem.value\n",
    "\n",
    "def soft_thresholding(y, lambd):\n",
    "    return np.sign(y) * np.maximum(np.abs(y) - lambd, 0)\n",
    "\n",
    "def proximal_gradient(A, b, gamma, stepsize, max_iters=1000, tol=1e-6):\n",
    "    n = A.shape[1]\n",
    "    x = np.zeros(n)\n",
    "    L = np.linalg.norm(A.T @ A, ord=2)\n",
    "    stepsize = min(stepsize, 1 / (2 * L))\n",
    "    history = $]\n",
    "    for k in range(max_iters):\n",
    "        grad = A.T @ (A @ x - b)\n",
    "        x_new = soft_thresholding(x - stepsize * grad, stepsize * gamma)\n",
    "        obj_value = 0.5 * np.linalg.norm(A @ x_new - b)**2 + gamma * np.linalg.norm(x_new, 1)\n",
    "        history.append(obj_value)\n",
    "        if np.any(np.isnan(x_new)) or np.any(np.isinf(x_new)):\n",
    "            print(\"Numerical instability detected. Stopping iteration.\")\n",
    "            break\n",
    "        if np.linalg.norm(x_new - x, ord=2) < tol:\n",
    "            break\n",
    "        x = x_new\n",
    "    return x, history\n",
    "\n",
    "def evaluate_proximal_gradient(m, n, gamma=0.1, stepsizes=[0.01, 0.05, 0.1]):\n",
    "    np.random.seed(0)\n",
    "    A = np.random.randn(m, n)\n",
    "    b = np.random.randn(m)\n",
    "    A = A / np.linalg.norm(A, ord=2)\n",
    "    b = b / np.linalg.norm(b, ord=2)\n",
    "\n",
    "    for stepsize in stepsizes:\n",
    "        start_time = time.time()\n",
    "        x_pg, history = proximal_gradient(A, b, gamma, stepsize)\n",
    "        pg_time = time.time() - start_time\n",
    "        print(f\"Proximal Gradient solution time (m={m}, n={n}, stepsize={stepsize}): {pg_time:.4f} seconds\")\n",
    "        print(f\"Proximal Gradient optimal value (m={m}, n={n}, stepsize={stepsize}): {history[-1]:.4f}\")\n",
    "\n",
    "def admm_lasso(A, b, gamma, rho, max_iters=1000, tol=1e-6):\n",
    "    m, n = A.shape\n",
    "    x = np.zeros(n)\n",
    "    y = np.zeros(n)\n",
    "    mu = np.zeros(n)\n",
    "    ATA = A.T @ A\n",
    "    ATb = A.T @ b\n",
    "    inv_matrix = la.inv(rho * np.eye(n) + ATA)\n",
    "    history = []\n",
    "\n",
    "    for k in range(max_iters):\n",
    "        x = inv_matrix @ (rho * y - mu + ATb)\n",
    "        y = soft_thresholding(x + mu / rho, gamma / rho)\n",
    "        mu += rho * (x - y)\n",
    "        obj_value = 0.5 * np.linalg.norm(A @ x - b)**2 + gamma * np.linalg.norm(x, 1)\n",
    "        history.append(obj_value)\n",
    "        if np.any(np.isnan(x)) or np.any(np.isinf(x)):\n",
    "            print(\"Numerical instability detected. Stopping iteration.\")\n",
    "            break\n",
    "        if np.linalg.norm(x - y, ord=2) < tol:\n",
    "            break\n",
    "    return x, history\n",
    "\n",
    "def evaluate_admm(m, n, gamma=0.1, rhos=[0.1, 1, 10]):\n",
    "    np.random.seed(0)\n",
    "    A = np.random.randn(m, n)\n",
    "    b = np.random.randn(m)\n",
    "    A = A / np.linalg.norm(A, ord=2)\n",
    "    b = b / np.linalg.norm(b, ord=2)\n",
    "\n",
    "    for rho in rhos:\n",
    "        start_time = time.time()\n",
    "        x_admm, history = admm_lasso(A, b, gamma, rho)\n",
    "        admm_time = time.time() - start_time\n",
    "        print(f\"ADMM solution time (m={m}, n={n}, rho={rho}): {admm_time:.4f} seconds\")\n",
    "        print(f\"ADMM optimal value (m={m}, n={n}, rho={rho}): {history[-1]:.4f}\")\n",
    "\n",
    "for m, n in [(100, 50), (200, 100), (300, 150)]:\n",
    "    print(f\"\\nEvaluating CVXPY for m={m}, n={n}\")\n",
    "    solve_with_cvxpy(m, n)\n",
    "\n",
    "    print(f\"\\nEvaluating Proximal Gradient for m={m}, n={n}\")\n",
    "    evaluate_proximal_gradient(m, n)\n",
    "\n",
    "    print(f\"\\nEvaluating ADMM for m={m}, n={n}\")\n",
    "    evaluate_admm(m, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CVXPY and ADMM for Graph Learning\n",
    "In this part of the script, the graph learning problem is solved using CVXPY and ADMM. The objective is to minimize the following function:\n",
    "\n",
    "$$\n",
    "\\min_{w \\geq 0} \\quad 2 z^T w - \\alpha \\sum \\log(Qw + 1e-6) + \\beta \\|w\\|_2^2\n",
    "$$\n",
    "\n",
    "where:\n",
    "- \\( $z$ \\) is a vector derived from the upper triangular part of a symmetric matrix \\( $Z$ \\).\n",
    "- \\( $Q$ \\) is a random matrix.\n",
    "- \\( $\\alpha, \\beta > 0$ \\) are regularization parameters.\n",
    "\n",
    "**CVXPY Solver:** The problem is formulated in CVXPY, and the solver directly minimizes the objective subject to the constraint \\( $w \\geq 0$ \\).\n",
    "\n",
    "**ADMM Solver:** The ADMM algorithm for this problem involves:\n",
    "1. **\\( $w$ \\)-update:**\n",
    "   $$\n",
    "   w^{k+1} = \\max\\left(w^k - \\tau_1 \\rho Q^T(Qw^k - v^k - \\frac{\\lambda^k}{\\rho}) - \\frac{2 \\tau_1 z}{2 \\tau_1 \\beta + 1}, 0\\right)\n",
    "   $$\n",
    "2. **\\( $v$ \\)-update:**\n",
    "   $$\n",
    "   v^{k+1} = \\frac{1}{2}\\left((1 - \\tau_2 \\rho)v^k + \\tau_2 \\rho Qw^{k+1} - \\tau_2 \\lambda^k + \\sqrt{((1 - \\tau_2 \\rho)v^k + \\tau_2 \\rho Qw^{k+1} - \\tau_2 \\lambda^k)^2 + 4 \\alpha \\tau_2}\\right)\n",
    "   $$\n",
    "3. **Dual variable update:**\n",
    "   $$\n",
    "   \\lambda^{k+1} = \\lambda^k + \\rho (Qw^{k+1} - v^{k+1})\n",
    "   $$\n",
    "\n",
    "The iterations continue until the primal and dual residuals fall below a specified tolerance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating CVXPY for N=30\n",
      "CVXPY solution time (N=30): 0.0163 seconds\n",
      "CVXPY optimal value (N=30): -62.1359\n",
      "\n",
      "Evaluating ADMM for N=30\n",
      "ADMM solution time (N=30): 0.2651 seconds\n",
      "ADMM optimal value (N=30): 41.4465\n",
      "\n",
      "Evaluating CVXPY for N=100\n",
      "CVXPY solution time (N=100): 0.8912 seconds\n",
      "CVXPY optimal value (N=100): -66.8370\n",
      "\n",
      "Evaluating ADMM for N=100\n",
      "ADMM solution time (N=100): 3.5950 seconds\n",
      "ADMM optimal value (N=100): 138.1551\n"
     ]
    }
   ],
   "source": [
    "def extract_upper_triangular(Z):\n",
    "    return Z[np.triu_indices_from(Z, k=1)]\n",
    "\n",
    "def solve_with_cvxpy(N, alpha=0.1, beta=0.01):\n",
    "    np.random.seed(0)\n",
    "    m = int(N * (N - 1) / 2)\n",
    "    Z = np.random.randn(N, N)\n",
    "    Z = (Z + Z.T) / 2\n",
    "    z = extract_upper_triangular(Z)\n",
    "    z = z / np.linalg.norm(z, ord=2)\n",
    "\n",
    "    Q = np.random.rand(N, m)\n",
    "    Q = Q / np.linalg.norm(Q, ord=2)\n",
    "\n",
    "    w = cp.Variable(m)\n",
    "    loss = 2 * z.T @ w - alpha * cp.sum(cp.log(Q @ w + 1e-6)) + beta * cp.norm(w, 2)**2\n",
    "    constraints = [w >= 0]\n",
    "    problem = cp.Problem(cp.Minimize(loss), constraints)\n",
    "\n",
    "    start_time = time.time()\n",
    "    problem.solve()\n",
    "    cvxpy_time = time.time() - start_time\n",
    "\n",
    "    print(f\"CVXPY solution time (N={N}): {cvxpy_time:.4f} seconds\")\n",
    "    print(f\"CVXPY optimal value (N={N}): {problem.value:.4f}\")\n",
    "    return problem.value\n",
    "\n",
    "def admm_graph_learning(z, Q, alpha, beta, rho, max_iters=1000, tol=1e-6):\n",
    "    m = len(z)\n",
    "    s = Q.shape[0]\n",
    "    w = np.zeros(m)\n",
    "    v = np.ones(s)\n",
    "    lam = np.zeros(s)\n",
    "\n",
    "    tau1 = 1 / (2 * (Q.shape[0] - 1) * rho)\n",
    "    tau2 = 1 / rho\n",
    "\n",
    "    for k in range(max_iters):\n",
    "        w_tilde = w - tau1 * rho * Q.T @ (Q @ w - v - lam / rho)\n",
    "        w_new = np.maximum(w_tilde - 2 * tau1 * z / (2 * tau1 * beta + 1), 0)\n",
    "\n",
    "        v_tilde = (1 - tau2 * rho) * v + tau2 * rho * Q @ w_new - tau2 * lam\n",
    "        v_tilde = np.clip(v_tilde, a_min=-1e3, a_max=1e3)\n",
    "        v_tilde_squared = v_tilde**2\n",
    "        v_new = (v_tilde + np.sqrt(v_tilde_squared + 4 * alpha * tau2)) / 2\n",
    "\n",
    "        lam += rho * (Q @ w_new - v_new)\n",
    "\n",
    "        rp = np.linalg.norm(Q @ w_new - v_new, ord=2)\n",
    "        rd = np.linalg.norm(Q.T @ (v_new - v), ord=2)\n",
    "\n",
    "        if rp < tol and rd < tol:\n",
    "            break\n",
    "\n",
    "        if np.any(np.isnan(w_new)) or np.any(np.isinf(w_new)) or np.any(np.isnan(v_new)) or np.any(np.isinf(v_new)):\n",
    "            print(\"Numerical instability detected. Stopping iteration.\")\n",
    "            break\n",
    "\n",
    "        w, v = w_new, v_new\n",
    "\n",
    "    return w\n",
    "\n",
    "\n",
    "def evaluate_admm(N, alpha=0.1, beta=0.01, rho=1.0, max_iters=1000, tol=1e-6):\n",
    "    np.random.seed(0)\n",
    "    m = int(N * (N - 1) / 2)\n",
    "    Z = np.random.randn(N, N)\n",
    "    Z = (Z + Z.T) / 2\n",
    "    z = extract_upper_triangular(Z)\n",
    "    z = z / np.linalg.norm(z, ord=2)\n",
    "\n",
    "    Q = np.random.rand(N, m)\n",
    "    Q = Q / np.linalg.norm(Q, ord=2)\n",
    "\n",
    "    start_time = time.time()\n",
    "    w_admm = admm_graph_learning(z, Q, alpha, beta, rho, max_iters, tol)\n",
    "    admm_time = time.time() - start_time\n",
    "\n",
    "    obj_admm = 2 * z.T @ w_admm - alpha * np.sum(np.log(Q @ w_admm + 1e-6)) + beta * np.linalg.norm(w_admm, 2)**2\n",
    "\n",
    "    print(f\"ADMM solution time (N={N}): {admm_time:.4f} seconds\")\n",
    "    print(f\"ADMM optimal value (N={N}): {obj_admm:.4f}\")\n",
    "    return obj_admm\n",
    "\n",
    "\n",
    "for N in [30, 100]:\n",
    "    print(f\"\\nEvaluating CVXPY for N={N}\")\n",
    "    solve_with_cvxpy(N)\n",
    "\n",
    "    print(f\"\\nEvaluating ADMM for N={N}\")\n",
    "    evaluate_admm(N, rho=1.0, max_iters=2000, tol=1e-6)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
